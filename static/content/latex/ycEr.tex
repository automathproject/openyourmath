\uuid{ycEr}
\titre{Regression linéaire}
\theme{Optimisation}
\auteur{Erwan HILLION}
\organisation{AMSCC}
\contenu{
\texte{On dispose d'observations $(x_1,y_1),\ldots,(x_n,y_n)$. On cherche les "meilleurs" coefficients $a$ et $b$ tels que pour chaque observation, on ait $y_i \approx a x_i + b$. Ce probl\`eme est appel\'e r\'egression lin\'eaire simple.

\medskip

Pour mesurer la qualit\'e des param\`etres $(a,b)$, on souhaite que l'\'ecart entre $y_i$ et $ax_i+b$ soit faible pour chaque observation. Pour quantifier l'erreur, on utilise le risque quadratique :
\begin{equation*}
R(a,b) = \sum_{i=1}^n (y_i - (a x_i+b) )^2.
\end{equation*}
Le probl\`eme est de minimiser la fonction $R(a,b)$. }

\medskip

\begin{enumerate}
\item \question{ Montrer que :
\begin{equation*}
R(a,b) = a^2 \sum_{i=1}^n x_i^2 + 2ab \sum_{i=1}^n x_i + b^2 n -2 a\sum_{i=1}^n x_i y_i -2 b \sum_{i=1}^n y_i + \sum_{i=1}^n y_i^2.
\end{equation*} }
\item \question{ Montrer que le gradient de $R$ s'écrit : 
\begin{equation} \label{eq:nablaR}
(\nabla R)(a,b) = \left( \begin{array}{c} 2a \sum_{i=1}^n x_i^2 + 2 b \sum_{i=1}^n x_i -2 \sum_{i=1}^n x_i y_i \\ 2a \sum_{i=1}^n x_i +2 b n -2 \sum_{i=1}^n y_i \end{array} \right).
\end{equation} }
\item \question{ Montrer que $R$ possède un unique point critique $(a^*,b^*)$ que l'on exprimera à l'aide des $x_i$ et des $y_i$.  }
\item \question{ Montrer que la hessienne de $R$ s'écrit :
\begin{equation*}
\textrm{Hess}_R(a,b)  = \left( \begin{array}{cc} 2 \sum_{i=1}^n x_i^2 & 2 \sum_{i=1}^n x_i \\ 2 \sum_{i=1}^n x_i & 2 n \end{array}\right).
\end{equation*}  }
\item \`A l'aide de la question pr\'ec\'edente et de l'in\'egalit\'e de Cauchy-Schwarz, montrer que la fonction $R$ est convexe.
\end{enumerate}

}
