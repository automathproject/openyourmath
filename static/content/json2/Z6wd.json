{
  "uuid": "Z6wd",
  "titre": "Méthode des moindres carrés",
  "theme": [
    "calcul différentiel",
    "optimisation"
  ],
  "niveau": "",
  "metadata": {
    "auteur": "",
    "createdAt": "2023-03-21",
    "organisation": "AMSCC",
    "updatedAt": "2025-01-18T15:03:39.517Z"
  },
  "contenu": [
    {
      "id": "94eedb12-2076-4472-94ea-42d2e23d07fd",
      "type": "description",
      "value": {
        "latex": "On considère des points $M_{1}, \\ldots, M_{n}$ de $\\mathbb{R}^{2}$, et on note $\\left(x_{i}, y_{i}\\right)$ les coordonnées de chaque point $M_{i}$.",
        "html": "<p>On considère des points <span class=\"math inline\">\\(M_{1}, \\ldots,\nM_{n}\\)</span> de <span class=\"math inline\">\\(\\mathbb{R}^{2}\\)</span>,\net on note <span class=\"math inline\">\\(\\left(x_{i},\ny_{i}\\right)\\)</span> les coordonnées de chaque point <span\nclass=\"math inline\">\\(M_{i}\\)</span>.</p>\n"
      }
    },
    {
      "id": "310a73b2-026f-4b7d-af3f-18423602d3a5",
      "type": "question",
      "value": {
        "latex": "On cherche les points $(x, y)$ de $\\mathbb{R}^{2}$ approchant au mieux le nuage de points formé par les points $M_{i}$ au sens des moindres carrés, c'est-à-dire qu'on cherche à minimiser la fonction\n\t\t$$\n\t\tf:(x, y) \\mapsto \\sum_{i=1}^{N}\\left(x-x_{i}\\right)^{2}+\\left(y-y_{i}\\right)^{2}\n\t\t$$\n\t\tOn admet que $f$ admet au moins un minimum global sur $\\mathbb{R}^{2}$. Déterminer en quels points $f$ admet ce minimum.",
        "html": "<p>On cherche les points <span class=\"math inline\">\\((x, y)\\)</span> de\n<span class=\"math inline\">\\(\\mathbb{R}^{2}\\)</span> approchant au mieux\nle nuage de points formé par les points <span\nclass=\"math inline\">\\(M_{i}\\)</span> au sens des moindres carrés,\nc’est-à-dire qu’on cherche à minimiser la fonction <span\nclass=\"math display\">\\[f:(x, y) \\mapsto\n\\sum_{i=1}^{N}\\left(x-x_{i}\\right)^{2}+\\left(y-y_{i}\\right)^{2}\\]</span>\nOn admet que <span class=\"math inline\">\\(f\\)</span> admet au moins un\nminimum global sur <span class=\"math inline\">\\(\\mathbb{R}^{2}\\)</span>.\nDéterminer en quels points <span class=\"math inline\">\\(f\\)</span> admet\nce minimum.</p>\n"
      }
    },
    {
      "id": "3712caa4-f0a9-4de0-983a-aa7fd78ea43c",
      "type": "reponse",
      "value": {
        "latex": "On cherche les points stationnaires : on trouve un point stationnaire $(x,y) = \\left(\\frac{1}{N}\\sum_{i=1}^N x_i,\\frac{1}{N}\\sum_{i=1}^N y_i \\right)$ et on reconnait le point moyen du nuage de points. On vérifie aisément qu'il s'agit d'un minimum local et qu'il est unique, c'est donc le minimum global.",
        "html": "<p>On cherche les points stationnaires : on trouve un point stationnaire\n<span class=\"math inline\">\\((x,y) = \\left(\\frac{1}{N}\\sum_{i=1}^N\nx_i,\\frac{1}{N}\\sum_{i=1}^N y_i \\right)\\)</span> et on reconnait le\npoint moyen du nuage de points. On vérifie aisément qu’il s’agit d’un\nminimum local et qu’il est unique, c’est donc le minimum global.</p>\n"
      }
    },
    {
      "id": "e84b7c07-e366-4a7e-83b8-3c24c054c002",
      "type": "question",
      "value": {
        "latex": "On cherche maintenant une relation affine entre les abscisses et les ordonnées de ces points. On cherche des constantes $m$ et $q$ pour que la droite d'équation $y=mx+q$ s'ajuste le mieux possible aux points observés. \n\t\t\n\t\tPour cela, on introduit $d_i = y_i - (mx_i + q)$ l'écart vertical du point $M_i$ par rapport à la droite. \n\t\t\n\t\tLa méthode des moindres carrés consiste à choisir $m$ et $q$ de telle sorte que la somme des écarts au carré soit minimale. \n\t\t\n\t\tExprimer $m$ et $q$ en fonction des coordonnées des points.",
        "html": "<p>On cherche maintenant une relation affine entre les abscisses et les\nordonnées de ces points. On cherche des constantes <span\nclass=\"math inline\">\\(m\\)</span> et <span\nclass=\"math inline\">\\(q\\)</span> pour que la droite d’équation <span\nclass=\"math inline\">\\(y=mx+q\\)</span> s’ajuste le mieux possible aux\npoints observés.</p>\n<p>Pour cela, on introduit <span class=\"math inline\">\\(d_i = y_i - (mx_i\n+ q)\\)</span> l’écart vertical du point <span\nclass=\"math inline\">\\(M_i\\)</span> par rapport à la droite.</p>\n<p>La méthode des moindres carrés consiste à choisir <span\nclass=\"math inline\">\\(m\\)</span> et <span\nclass=\"math inline\">\\(q\\)</span> de telle sorte que la somme des écarts\nau carré soit minimale.</p>\n<p>Exprimer <span class=\"math inline\">\\(m\\)</span> et <span\nclass=\"math inline\">\\(q\\)</span> en fonction des coordonnées des\npoints.</p>\n"
      }
    },
    {
      "id": "467f081c-46bb-4b10-83e9-706a794a4ad6",
      "type": "reponse",
      "value": {
        "latex": "Pour cela, on doit minimiser la fonction $\\mathscr{E}: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}_{+}$définie par\n\t\t\t$$\n\t\t\t\\mathscr{E}(m, q)=\\sum_{i=0}^{n} d_{i}^{2}=\\sum_{i=0}^{n}\\left(y_{i}-m x_{i}-q\\right)^{2}\n\t\t\t$$\n\t\t\tPour minimiser $\\mathscr{E}$ on cherche d'abord les points stationnaires, i.e. les points $(m, q)$ qui vérifient $\\frac{\\partial \\mathscr{E}}{\\partial m}=\\frac{\\partial \\mathscr{E}}{\\partial q}=0$. Puisque\n\t\t\t$$\n\t\t\t\\frac{\\partial \\mathscr{E}}{\\partial m}(m, q)=-2\\left(\\sum_{i=0}^{n}\\left(y_{i}-\\left(m x_{i}+q\\right)\\right) x_{i}\\right), \\quad \\frac{\\partial \\mathscr{E}}{\\partial q}(m, q)=-2\\left(\\sum_{i=0}^{n}\\left(y_{i}-\\left(m x_{i}+q\\right)\\right)\\right),\n\t\t\t$$\n\t\t\t$$\n\t\t\t\\left\\{\\begin{array} { l } \n\t\t\t\t{ \\frac { \\partial \\mathscr { E } } { \\partial m } ( m , q ) = 0 } \\\\\n\t\t\t\t{ \\frac { \\partial \\mathscr { E } } { \\partial q } ( m , q ) = 0 }\n\t\t\t\\end{array} \\Longleftrightarrow \\left\\{\\begin{array}{l}\n\t\t\t\t\\sum_{i=0}^{n}\\left(y_{i}-m x_{i}-q\\right) x_{i}=0 \\\\\n\t\t\t\t\\sum_{i=0}^{n}\\left(y_{i}-m x_{i}-q\\right)=0\n\t\t\t\\end{array}\\right.\\right.\n\t\t\t$$\n\t\t\t$$\n\t\t\t\\Longleftrightarrow\\left\\{\\begin{array} { l } \n\t\t\t\t{ ( \\sum _ { i = 1 } ^ { n } x _ { i } ^ { 2 } ) m + ( \\sum _ { i = 0 } ^ { n } x _ { i } ) q = \\sum _ { i = 0 } ^ { n } y _ { i } x _ { i } } \\\\\n\t\t\t\t{ ( \\sum _ { i = 1 } ^ { n } x _ { i } ) m + ( n + 1 ) q = \\sum _ { i = 0 } ^ { n } y _ { i } }\n\t\t\t\\end{array} \\Longleftrightarrow \\left\\{\\begin{array}{l}\n\t\t\t\tm=\\frac{\\left(\\sum_{i=0}^{n} x_{i}\\right)\\left(\\sum_{i=0}^{n} y_{i}\\right)-(n+1)\\left(\\sum_{i=0}^{n} x_{i} y_{i}\\right)}{\\left(\\sum_{i=0}^{n} x_{i}\\right)^{2}-(n+1)\\left(\\sum_{i=0}^{n} x_{i}^{2}\\right)}, \\\\\n\t\t\t\tq=\\frac{\\left(\\sum_{i=0}^{n} x_{i}\\right)\\left(\\sum_{i=0}^{n} x_{i} y_{i}\\right)-\\left(\\sum_{i=0}^{n} y_{i}\\right)\\left(\\sum_{i=0}^{n} x_{i}^{2}\\right)}{\\left(\\sum_{i=0}^{n} x_{i}\\right)^{2}-(n+1)\\left(\\sum_{i=0}^{n} x_{i}^{2}\\right)} .\n\t\t\t\\end{array}\\right.\\right.\n\t\t\t$$\n\t\t\tOn a trouvé un seul point stationnaire. On établi sa nature en étudiant la matrice Hessienne :\n\t\t\t$$\n\t\t\tH_{\\mathscr{E}}(m, q)=2\\left(\\begin{array}{cc}\n\t\t\t\t\\sum_{i=1}^{n} x_{i}^{2} & \\sum_{i=0}^{n} x_{i} \\\\\n\t\t\t\t\\sum_{i=0}^{n} x_{i} & (n+1)\n\t\t\t\\end{array}\\right)\n\t\t\t$$\n\t\t\tet $\\operatorname{det}\\left(H_{\\mathscr{E}}(m, q)\\right)=4\\left((n+1) \\sum_{i=1}^{n} x_{i}^{2}-\\left(\\sum_{i=0}^{n} x_{i}\\right)^{2}\\right)>0$ avec $\\partial_{m m} \\mathscr{E}(m, q)=\\sum_{i=1}^{n} x_{i}^{2}>0$ donc il s'agit bien d'un minimum. La droite d'équation $y=m x+q$ ainsi calculée s'appelle droite de régression de y par rapport à $x$.",
        "html": "<p>Pour cela, on doit minimiser la fonction <span\nclass=\"math inline\">\\(\\mathscr{E}: \\mathbb{R}^{2} \\rightarrow\n\\mathbb{R}_{+}\\)</span>définie par <span\nclass=\"math display\">\\[\\mathscr{E}(m, q)=\\sum_{i=0}^{n}\nd_{i}^{2}=\\sum_{i=0}^{n}\\left(y_{i}-m x_{i}-q\\right)^{2}\\]</span> Pour\nminimiser <span class=\"math inline\">\\(\\mathscr{E}\\)</span> on cherche\nd’abord les points stationnaires, i.e. les points <span\nclass=\"math inline\">\\((m, q)\\)</span> qui vérifient <span\nclass=\"math inline\">\\(\\frac{\\partial \\mathscr{E}}{\\partial\nm}=\\frac{\\partial \\mathscr{E}}{\\partial q}=0\\)</span>. Puisque <span\nclass=\"math display\">\\[\\frac{\\partial \\mathscr{E}}{\\partial m}(m,\nq)=-2\\left(\\sum_{i=0}^{n}\\left(y_{i}-\\left(m x_{i}+q\\right)\\right)\nx_{i}\\right), \\quad \\frac{\\partial \\mathscr{E}}{\\partial q}(m,\nq)=-2\\left(\\sum_{i=0}^{n}\\left(y_{i}-\\left(m\nx_{i}+q\\right)\\right)\\right),\\]</span> <span\nclass=\"math display\">\\[\\left\\{\\begin{array} { l }\n                { \\frac { \\partial \\mathscr { E } } { \\partial m } ( m ,\nq ) = 0 } \\\\\n                { \\frac { \\partial \\mathscr { E } } { \\partial q } ( m ,\nq ) = 0 }\n            \\end{array} \\Longleftrightarrow \\left\\{\\begin{array}{l}\n                \\sum_{i=0}^{n}\\left(y_{i}-m x_{i}-q\\right) x_{i}=0 \\\\\n                \\sum_{i=0}^{n}\\left(y_{i}-m x_{i}-q\\right)=0\n            \\end{array}\\right.\\right.\\]</span> <span\nclass=\"math display\">\\[\\Longleftrightarrow\\left\\{\\begin{array} { l }\n                { ( \\sum _ { i = 1 } ^ { n } x _ { i } ^ { 2 } ) m + (\n\\sum _ { i = 0 } ^ { n } x _ { i } ) q = \\sum _ { i = 0 } ^ { n } y _ {\ni } x _ { i } } \\\\\n                { ( \\sum _ { i = 1 } ^ { n } x _ { i } ) m + ( n + 1 ) q\n= \\sum _ { i = 0 } ^ { n } y _ { i } }\n            \\end{array} \\Longleftrightarrow \\left\\{\\begin{array}{l}\n                m=\\frac{\\left(\\sum_{i=0}^{n}\nx_{i}\\right)\\left(\\sum_{i=0}^{n} y_{i}\\right)-(n+1)\\left(\\sum_{i=0}^{n}\nx_{i} y_{i}\\right)}{\\left(\\sum_{i=0}^{n}\nx_{i}\\right)^{2}-(n+1)\\left(\\sum_{i=0}^{n} x_{i}^{2}\\right)}, \\\\\n                q=\\frac{\\left(\\sum_{i=0}^{n}\nx_{i}\\right)\\left(\\sum_{i=0}^{n} x_{i} y_{i}\\right)-\\left(\\sum_{i=0}^{n}\ny_{i}\\right)\\left(\\sum_{i=0}^{n} x_{i}^{2}\\right)}{\\left(\\sum_{i=0}^{n}\nx_{i}\\right)^{2}-(n+1)\\left(\\sum_{i=0}^{n} x_{i}^{2}\\right)} .\n            \\end{array}\\right.\\right.\\]</span> On a trouvé un seul point\nstationnaire. On établi sa nature en étudiant la matrice Hessienne :\n<span class=\"math display\">\\[H_{\\mathscr{E}}(m,\nq)=2\\left(\\begin{array}{cc}\n                \\sum_{i=1}^{n} x_{i}^{2} &amp; \\sum_{i=0}^{n} x_{i} \\\\\n                \\sum_{i=0}^{n} x_{i} &amp; (n+1)\n            \\end{array}\\right)\\]</span> et <span\nclass=\"math inline\">\\(\\operatorname{det}\\left(H_{\\mathscr{E}}(m,\nq)\\right)=4\\left((n+1) \\sum_{i=1}^{n} x_{i}^{2}-\\left(\\sum_{i=0}^{n}\nx_{i}\\right)^{2}\\right)&gt;0\\)</span> avec <span\nclass=\"math inline\">\\(\\partial_{m m} \\mathscr{E}(m, q)=\\sum_{i=1}^{n}\nx_{i}^{2}&gt;0\\)</span> donc il s’agit bien d’un minimum. La droite\nd’équation <span class=\"math inline\">\\(y=m x+q\\)</span> ainsi calculée\ns’appelle droite de régression de y par rapport à <span\nclass=\"math inline\">\\(x\\)</span>.</p>\n"
      }
    }
  ]
}