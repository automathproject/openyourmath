{
  "uuid": "ycEr",
  "titre": "Regression linéaire",
  "theme": [
    "optimisation"
  ],
  "niveau": "",
  "metadata": {
    "auteur": "Erwan HILLION",
    "createdAt": "2024-11-23T17:28:00.454Z",
    "organisation": "AMSCC",
    "updatedAt": "2024-11-23T17:28:00.454Z"
  },
  "contenu": [
    {
      "id": "5c3fc8c0-6c0e-4a9e-9fe8-2a4bd1446b4b",
      "type": "description",
      "value": {
        "latex": "On dispose d'observations $(x_1,y_1),\\ldots,(x_n,y_n)$. On cherche les \"meilleurs\" coefficients $a$ et $b$ tels que pour chaque observation, on ait $y_i \\approx a x_i + b$. Ce probl\\`eme est appel\\'e r\\'egression lin\\'eaire simple.\n\n\\medskip\n\nPour mesurer la qualit\\'e des param\\`etres $(a,b)$, on souhaite que l'\\'ecart entre $y_i$ et $ax_i+b$ soit faible pour chaque observation. Pour quantifier l'erreur, on utilise le risque quadratique :\n\\begin{equation*}\nR(a,b) = \\sum_{i=1}^n (y_i - (a x_i+b) )^2.\n\\end{equation*}\nLe probl\\`eme est de minimiser la fonction $R(a,b)$.",
        "html": "<p>On dispose d’observations <span\nclass=\"math inline\">\\((x_1,y_1),\\ldots,(x_n,y_n)\\)</span>. On cherche\nles \"meilleurs\" coefficients <span class=\"math inline\">\\(a\\)</span> et\n<span class=\"math inline\">\\(b\\)</span> tels que pour chaque observation,\non ait <span class=\"math inline\">\\(y_i \\approx a x_i + b\\)</span>. Ce\nproblème est appelé régression linéaire simple.</p>\n<p>Pour mesurer la qualité des paramètres <span\nclass=\"math inline\">\\((a,b)\\)</span>, on souhaite que l’écart entre\n<span class=\"math inline\">\\(y_i\\)</span> et <span\nclass=\"math inline\">\\(ax_i+b\\)</span> soit faible pour chaque\nobservation. Pour quantifier l’erreur, on utilise le risque quadratique\n: <span class=\"math display\">\\[R(a,b) = \\sum_{i=1}^n (y_i - (a x_i+b)\n)^2.\\]</span> Le problème est de minimiser la fonction <span\nclass=\"math inline\">\\(R(a,b)\\)</span>.</p>\n"
      }
    },
    {
      "id": "875e805a-a5fb-4cf9-9a57-e5a1548d64c4",
      "type": "question",
      "value": {
        "latex": "Montrer que :\n\\begin{equation*}\nR(a,b) = a^2 \\sum_{i=1}^n x_i^2 + 2ab \\sum_{i=1}^n x_i + b^2 n -2 a\\sum_{i=1}^n x_i y_i -2 b \\sum_{i=1}^n y_i + \\sum_{i=1}^n y_i^2.\n\\end{equation*}",
        "html": "<p>Montrer que : <span class=\"math display\">\\[R(a,b) = a^2 \\sum_{i=1}^n\nx_i^2 + 2ab \\sum_{i=1}^n x_i + b^2 n -2 a\\sum_{i=1}^n x_i y_i -2 b\n\\sum_{i=1}^n y_i + \\sum_{i=1}^n y_i^2.\\]</span></p>\n"
      }
    },
    {
      "id": "1f116aa2-a262-4473-b8f1-c0b2503ee1c4",
      "type": "reponse",
      "value": {
        "latex": "La formule s'obtient simplement en développant chaque expression de la somme.",
        "html": "<p>La formule s’obtient simplement en développant chaque expression de\nla somme.</p>\n"
      }
    },
    {
      "id": "3197db67-2de6-436b-b89e-925b10e55ed5",
      "type": "question",
      "value": {
        "latex": "Montrer que le gradient de $R$ s'écrit : \n\\begin{equation} \\label{eq:nablaR}\n(\\nabla R)(a,b) = \\left( \\begin{array}{c} 2a \\sum_{i=1}^n x_i^2 + 2 b \\sum_{i=1}^n x_i -2 \\sum_{i=1}^n x_i y_i \\\\ 2a \\sum_{i=1}^n x_i +2 b n -2 \\sum_{i=1}^n y_i \\end{array} \\right).\n\\end{equation}",
        "html": "<p>Montrer que le gradient de <span class=\"math inline\">\\(R\\)</span>\ns’écrit : <span class=\"math display\">\\[\\label{eq:nablaR}\n(\\nabla R)(a,b) = \\left( \\begin{array}{c} 2a \\sum_{i=1}^n x_i^2 + 2 b\n\\sum_{i=1}^n x_i -2 \\sum_{i=1}^n x_i y_i \\\\ 2a \\sum_{i=1}^n x_i +2 b n\n-2 \\sum_{i=1}^n y_i \\end{array} \\right).\\]</span></p>\n"
      }
    },
    {
      "id": "94758dfa-8a1a-45a9-b661-4b959a9f8214",
      "type": "reponse",
      "value": {
        "latex": "Cela découle d'un calcul direct.",
        "html": "<p>Cela découle d’un calcul direct.</p>\n"
      }
    },
    {
      "id": "a5677b35-6301-4c08-952e-ab5a0ce335b9",
      "type": "question",
      "value": {
        "latex": "Montrer que $R$ possède un unique point critique $(a^*,b^*)$ que l'on exprimera à l'aide des $x_i$ et des $y_i$.",
        "html": "<p>Montrer que <span class=\"math inline\">\\(R\\)</span> possède un unique\npoint critique <span class=\"math inline\">\\((a^*,b^*)\\)</span> que l’on\nexprimera à l’aide des <span class=\"math inline\">\\(x_i\\)</span> et des\n<span class=\"math inline\">\\(y_i\\)</span>.</p>\n"
      }
    },
    {
      "id": "7521965b-c3fd-4c77-ae8e-6e2c65eb196e",
      "type": "reponse",
      "value": {
        "latex": "On cherche $(a^*,b^*)$ tel que $\\nabla R(a^*,b^*)=0$. Au vu de l'expression explicite du gradient, on s'aperçoit qu'il faut résoudre un système linéaire à deux inconnues. La résolution de ce système (par exemple avec le pivot de Gauss) donne l'unique solution : $$a^* = \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i - n \\sum_{i=1}^n x_iy_i}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n x_i^2} \\ , \\ b^*=\\frac{\\sum_{i=1}^n x_iy_i \\sum_{i=1}^n x_i-\\sum_{i=1}^n y_i \\sum_{i=1}^n x_i^2}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n x_i^2}.$$",
        "html": "<p>On cherche <span class=\"math inline\">\\((a^*,b^*)\\)</span> tel que\n<span class=\"math inline\">\\(\\nabla R(a^*,b^*)=0\\)</span>. Au vu de\nl’expression explicite du gradient, on s’aperçoit qu’il faut résoudre un\nsystème linéaire à deux inconnues. La résolution de ce système (par\nexemple avec le pivot de Gauss) donne l’unique solution : <span\nclass=\"math display\">\\[a^* = \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i - n\n\\sum_{i=1}^n x_iy_i}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n\nx_i^2} \\ , \\ b^*=\\frac{\\sum_{i=1}^n x_iy_i \\sum_{i=1}^n x_i-\\sum_{i=1}^n\ny_i \\sum_{i=1}^n x_i^2}{\\left(\\sum_{i=1}^n x_i\\right)^2-n\\sum_{i=1}^n\nx_i^2}.\\]</span></p>\n"
      }
    },
    {
      "id": "5ae6a237-a0a9-4e46-81f6-2790f7fdd2e6",
      "type": "question",
      "value": {
        "latex": "Montrer que la hessienne de $R$ s'écrit :\n\\begin{equation*}\n\\textrm{Hess}_R(a,b)  = \\left( \\begin{array}{cc} 2 \\sum_{i=1}^n x_i^2 & 2 \\sum_{i=1}^n x_i \\\\ 2 \\sum_{i=1}^n x_i & 2 n \\end{array}\\right).\n\\end{equation*}",
        "html": "<p>Montrer que la hessienne de <span class=\"math inline\">\\(R\\)</span>\ns’écrit : <span class=\"math display\">\\[\\textrm{Hess}_R(a,b)  = \\left(\n\\begin{array}{cc} 2 \\sum_{i=1}^n x_i^2 &amp; 2 \\sum_{i=1}^n x_i \\\\ 2\n\\sum_{i=1}^n x_i &amp; 2 n \\end{array}\\right).\\]</span></p>\n"
      }
    },
    {
      "id": "a6c979c7-9918-4ba2-8757-81fa8610f250",
      "type": "reponse",
      "value": {
        "latex": "La hessienne s'obtient par calcul direct, en dérivant les dérivées partielles obtenues dans le calcul du gradient.",
        "html": "<p>La hessienne s’obtient par calcul direct, en dérivant les dérivées\npartielles obtenues dans le calcul du gradient.</p>\n"
      }
    },
    {
      "id": "4c94082d-d288-49cc-8df1-07d9c7623c88",
      "type": "reponse",
      "value": {
        "latex": "On peut remarquer que la hessienne est constante en $a$ et $b$ (car la fonciton $R$ est polynomiale de degré $2$). Il suffit donc de montrer que $\\langle X, H X \\rangle \\ge 0$ pour tout vecteur $X = \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\in \\R^2$, où $H$ est la hessienne obtenue précédemment. Ce produit scalaire s'écrit, après développement : $$ \\frac{1}{4} \\langle X, H X \\rangle = a^2 \\sum_{i=1}^n x_i^2 + 2 a b \\sum_{i=1}^n x_i  + b^2 n.$$ Par Cauchy-Schwarz, on a $\\sum_{i=1}^n x_i^2 \\ge \\frac{1}{n} \\left(\\sum_{i=1}^n x_i \\right)^2$, d'où il vient : $$ \\frac{1}{4} \\langle X, H X \\rangle  \\ge \\frac{1}{n}\\left(a \\sum_{i=1}^n x_i + b n \\right)^2 \\ge 0,$$ comme voulu.",
        "html": "<p>On peut remarquer que la hessienne est constante en <span\nclass=\"math inline\">\\(a\\)</span> et <span\nclass=\"math inline\">\\(b\\)</span> (car la fonciton <span\nclass=\"math inline\">\\(R\\)</span> est polynomiale de degré <span\nclass=\"math inline\">\\(2\\)</span>). Il suffit donc de montrer que <span\nclass=\"math inline\">\\(\\langle X, H X \\rangle \\ge 0\\)</span> pour tout\nvecteur <span class=\"math inline\">\\(X = \\begin{pmatrix} a \\\\ b\n\\end{pmatrix} \\in \\R^2\\)</span>, où <span\nclass=\"math inline\">\\(H\\)</span> est la hessienne obtenue précédemment.\nCe produit scalaire s’écrit, après développement : <span\nclass=\"math display\">\\[\\frac{1}{4} \\langle X, H X \\rangle = a^2\n\\sum_{i=1}^n x_i^2 + 2 a b \\sum_{i=1}^n x_i  + b^2 n.\\]</span> Par\nCauchy-Schwarz, on a <span class=\"math inline\">\\(\\sum_{i=1}^n x_i^2 \\ge\n\\frac{1}{n} \\left(\\sum_{i=1}^n x_i \\right)^2\\)</span>, d’où il vient :\n<span class=\"math display\">\\[\\frac{1}{4} \\langle X, H X \\rangle  \\ge\n\\frac{1}{n}\\left(a \\sum_{i=1}^n x_i + b n \\right)^2 \\ge 0,\\]</span>\ncomme voulu.</p>\n"
      }
    }
  ]
}